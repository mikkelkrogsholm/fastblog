{
  
    
        "post0": {
            "title": "Last Christmas I gave you my heart",
            "content": "This blogpost looks at music. . I have downloaded most of the chart data from spotifycharts and stored them in a SQLite database. . Specifically I will look at &quot;seasonal classics&quot;. It turns out that there are no summer classics - only christmas classics. It looks like Summer Hits are temporary and fleeing where Christmas songs are more permanent. . Setup . First I load the needed libraries and created a connection to my local database with music. . suppressPackageStartupMessages(library(tidyverse)) library(RSQLite) options(repr.plot.width=15, repr.plot.height=7.5, scipen = 999) con &lt;- dbConnect(RSQLite::SQLite(), &quot;spotify.db&quot;) . Collect the data . I will look a three countries: Germany (de), Denmark (dk) and Great Britain (gb). And then create a plot that shows how many Christmas songs are the same year on year, but summer songs are not. . # Make a reference to the table in the DB tblref &lt;- tbl(con, &quot;top200&quot;) . # Collect the data dfhere &lt;- tblref %&gt;% filter(region %in% c(&quot;de&quot;, &quot;dk&quot;, &quot;gb&quot;)) %&gt;% # filter relevant countries mutate(m = str_sub(date, 6, 7), # create a month column y = str_sub(date, 1, 4), # create a year column ym = paste(y,m)) %&gt;% # create a yearmonth column group_by(region, y, m, ym, `Track Name`, Artist, URL) %&gt;% # Group by region, track and year month summarise(streams = sum(Streams, na.rm = TRUE)) %&gt;% # sum up the streams for a track per year month ungroup() %&gt;% # ungroup collect() # Import the data into R&#39;s memory # Lets have a look at the first few lines head(dfhere) . With the monthly data now in memory I will pick the top 100 tracks per month . top &lt;- dfhere %&gt;% group_by(region, y, m, ym) %&gt;% top_n(n = 100, wt = streams) %&gt;% ungroup() # yms &lt;- top$ym %&gt;% unique() %&gt;% sort() # top$ym &lt;- factor(top$ym, levels = yms) head(top) . Now we need to divide the data into different data frames. We need these to create the visualization. . # First I create a table with repeaters, i.e. with songs that appear in the same month across the years more than once: repeaters &lt;- top %&gt;% group_by(region) %&gt;% count(m, `Track Name`, Artist, URL) %&gt;% filter(n &gt; 1) %&gt;% select(region, URL) %&gt;% ungroup() # Then I filter the data to only contain data for the repeathers toprep &lt;- top %&gt;% inner_join(repeaters, by = c(&quot;region&quot;, &quot;URL&quot;)) # Then I create a table with the songs that have max hits in december xmasmax &lt;- toprep %&gt;% group_by(region, URL) %&gt;% filter(streams == max(streams), m == 12) %&gt;% select(region, URL) %&gt;% ungroup() # And use this table to create a data frame for repeater songs where # the most streams are not in december noxmas &lt;- toprep %&gt;% anti_join(xmasmax, by = c(&quot;region&quot;, &quot;URL&quot;)) %&gt;% distinct() # And a dataframe where most streams are in december xmas &lt;- toprep %&gt;% inner_join(xmasmax, by = c(&quot;region&quot;, &quot;URL&quot;)) %&gt;% distinct() head(xmas) . Visualize the data . Year on year repeaters . Let&#39;s first look at the songs that repeat year on year. As you can see the songs normally have a more or less sharp rise and then a long slow decline. It is this decline that makes it possible for the song to appear in the same months year on year, although on a very different scale. . # I need a little helper to create pretty x-axis on the charts yms &lt;- top$ym %&gt;% unique() %&gt;% sort() yms_labs &lt;- str_c(str_sub(yms, -2,-1), &quot; &#39;&quot;, str_sub(yms, 3, 4)) yms_df &lt;- top %&gt;% distinct(ym) . ggplot() + # Then I plot the non-xmas data in green geom_line(data = noxmas, aes(ym, streams, group = URL), show.legend = FALSE, color = &quot;darkgreen&quot;, linetype = &quot;dashed&quot;, alpha = .4) + geom_point(data = noxmas, aes(ym, streams, group = URL), show.legend = FALSE, color = &quot;darkgreen&quot;, alpha = .4) + # And some chart styling scale_x_discrete(labels = yms_labs, limits = yms) + theme_minimal() + theme(axis.text.x = element_text(angle = 45)) + facet_wrap(facets = ~ region, ncol = 1, scales = &quot;free_y&quot;) + labs(x = &quot;&quot;, y = &quot;Streams per month&quot;) . Christmas repeaters . It looks a bit different if we plot the Christmas songs. It becomes very obvious that the Christmas repeaters are limited to Christmas. They also do not follow the same path as the other songs (sharp increase, long slow decrease). They come to life pretty high on the charts in december (some in november) and then they quickly die out and wait for Christmas to come again. I have marked decemberr with a red dashed line. . ggplot() + # First I create a vertical red dashed line for every december month geom_vline(xintercept = c(&quot;2017 12&quot;, &quot;2018 12&quot;, &quot;2019 12&quot;), linetype = &quot;dashed&quot;, color = &quot;red&quot;) + # And the christmas data in blue geom_line(data = xmas, aes(ym, streams, group = URL), show.legend = FALSE, color = &quot;blue&quot;, alpha = .5) + geom_point(data = xmas, aes(ym, streams, group = URL), show.legend = FALSE, color = &quot;blue&quot;, alpha = .9) + # And some chart styling scale_x_discrete(labels = yms_labs, limits = yms) + theme_minimal() + theme(axis.text.x = element_text(angle = 45)) + facet_wrap(facets = ~ region, ncol = 1, scales = &quot;free_y&quot;) + labs(x = &quot;&quot;, y = &quot;Streams per month&quot;) . Plotted together . When plotted together the difference becomes obvious. . ggplot() + # First I create a vertical red dashed line for every december month geom_vline(xintercept = c(&quot;2017 12&quot;, &quot;2018 12&quot;, &quot;2019 12&quot;), linetype = &quot;dashed&quot;, color = &quot;red&quot;) + # Then I plot the non-xmas data in green geom_line(data = noxmas, aes(ym, streams, group = URL), show.legend = FALSE, color = &quot;darkgreen&quot;, linetype = &quot;dashed&quot;, alpha = .4) + geom_point(data = noxmas, aes(ym, streams, group = URL), show.legend = FALSE, color = &quot;darkgreen&quot;, alpha = .4) + # And the christmas data in blue geom_line(data = xmas, aes(ym, streams, group = URL), show.legend = FALSE, color = &quot;blue&quot;, alpha = .5) + geom_point(data = xmas, aes(ym, streams, group = URL), show.legend = FALSE, color = &quot;blue&quot;, alpha = .9) + # And some chart styling scale_x_discrete(labels = yms_labs, limits = yms) + theme_minimal() + theme(axis.text.x = element_text(angle = 45)) + facet_wrap(facets = ~ region, ncol = 1, scales = &quot;free_y&quot;) + labs(x = &quot;&quot;, y = &quot;Streams per month&quot;) . Interpretation . Non Christmas . We see a few non-christmas songs that make it more than a year - ie they appear in the same month i more than one year. But, they show a much different trend than the christmas songs. They have a pattern of popularity and then a fade out. . Christmas . With the Christmas songs we see a whole different pattern. First we see more of them, ie there are more survivors year on year. Second the pattern is quite different; the songs lie dormant pretty much all year and then suddenly they explode in december. Some even get an early listen in november, where the must christmas enthusiastic people get started. . Conlusion . There is no such thing as a summer classic - but plenty of christmas classics! .",
            "url": "https://mikkelkrogsholm.github.io/fastblog/spotify/charts/christmas/summer/r/jupyter/2020/08/05/last_christmas.html",
            "relUrl": "/spotify/charts/christmas/summer/r/jupyter/2020/08/05/last_christmas.html",
            "date": " • Aug 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Use data to find the best audio books",
            "content": "I am a ferocious audiobook consumer and I finish 40-60 books a year (the trick is to listen to them a 2x-3x the speed). I am also picky. There is a lot of great content out there and I only want to listen to the best of it. . I have a trial membership of the audiobook service Nextory. I wasn&#39;t completely satistified with the way their filtering works, so I decided to see if I could access their data and create my own filtering. It turns out I could. . This blog shows you how, so you can create a list of great audiobooks to listen to! . Import libraries . We need to import a few libraries for the code to work. . # For doinng api calls import requests from requests.exceptions import HTTPError # For creating data frames import pandas as pd # For cleaning text import re # For doing math import numpy as np # For showing a progress bar from tqdm import tqdm . Get the data . It turns out that the Nextory webpage does API requests to their database behind the scenes, and by mimicking those, we can get the data in a nice clean format that is ready to be analysed. . We can use the requests library to mimick the api calls. . Custom functions . I am defining three custom functions that I need to either retrieve data or clean data. . A get_categories function that gets all of the &quot;main&quot; categories that Nextory has. | A get_books function that gets n amount of books in a category. | A clean_cat function that cleans the category label and makes it pretty. | . # Define a custom function for getting book categories def get_categories(): headers = { &#39;Authorization&#39;: &#39;Basic bmV4dG9yeXVpOnRvYmVkZWNpZGVk&#39;, &#39;locale&#39;: &#39;da_DK&#39;, &#39;user-agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36&#39; } url = &#39;https://www.nextory.dk/api/web/catalogue/6.2/groups?view=category&#39; try: response = requests.get(url, headers = headers) response.raise_for_status() # access JSON content jsonResponse = response.json() bookgroups = jsonResponse[&#39;data&#39;][&#39;bookgroups&#39;] return bookgroups except HTTPError as http_err: print(f&#39;HTTP error occurred: {http_err}&#39;) except Exception as err: print(f&#39;Other error occurred: {err}&#39;) # Define a custom function for getting books def get_books(category, rows = 20): headers = { &#39;Authorization&#39;: &#39;Basic bmV4dG9yeXVpOnRvYmVkZWNpZGVk&#39;, &#39;locale&#39;: &#39;da_DK&#39;, &#39;user-agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36&#39; } url = &quot;https://www.nextory.dk/api/web/catalogue/6.2/booksforbookgroup/&quot; + category + &quot;?rows=&quot; + str(rows) try: response = requests.get(url, headers = headers) response.raise_for_status() # access JSON content jsonResponse = response.json() books = jsonResponse[&#39;data&#39;][&#39;books&#39;] return books except HTTPError as http_err: print(f&#39;HTTP error occurred: {http_err}&#39;) except Exception as err: print(f&#39;Other error occurred: {err}&#39;) # Define a custom function for cleaning categories def clean_cat(s): s = re.sub(r&#39;- d$&#39;, &#39;&#39;, s) s = re.sub(r&#39;-&#39;, &#39; &#39;, s) s = s.title() return s . Get the book categories . First I run my get_categories function to get that categories. . bookcats = get_categories() . This returns a list of dictionaries with information about each category. Lets look at the first entry. It has some information about the &quot;main&quot; category and then a list of subcategories within the main one. . NOTE: As you can tell the names are in Danish. You can change the the locale for Nextory in the custom functions above to get data for another region. . bookcats[0] . {&#39;id&#39;: &#39;4001&#39;, &#39;title&#39;: &#39;Krimi&#39;, &#39;position&#39;: 1, &#39;slugname&#39;: &#39;krimi-1&#39;, &#39;subcategories&#39;: [{&#39;id&#39;: &#39;4002&#39;, &#39;title&#39;: &#39;Hyggelig krimi&#39;, &#39;position&#39;: 1, &#39;slugname&#39;: &#39;hyggelig-krimi-2&#39;}, {&#39;id&#39;: &#39;4003&#39;, &#39;title&#39;: &#39;Humoristisk krimi&#39;, &#39;position&#39;: 2, &#39;slugname&#39;: &#39;humoristisk-krimi-2&#39;}, {&#39;id&#39;: &#39;4004&#39;, &#39;title&#39;: &#39;Privatdetektiver&#39;, &#39;position&#39;: 3, &#39;slugname&#39;: &#39;privatdetektiver-2&#39;}, {&#39;id&#39;: &#39;4005&#39;, &#39;title&#39;: &#39;Noir krimi&#39;, &#39;position&#39;: 4, &#39;slugname&#39;: &#39;noir-krimi-2&#39;}, {&#39;id&#39;: &#39;4006&#39;, &#39;title&#39;: &#39;Politi romaner&#39;, &#39;position&#39;: 5, &#39;slugname&#39;: &#39;politi-romaner-2&#39;}, {&#39;id&#39;: &#39;4007&#39;, &#39;title&#39;: &#39;Klassisk krimi&#39;, &#39;position&#39;: 6, &#39;slugname&#39;: &#39;klassisk-krimi-2&#39;}, {&#39;id&#39;: &#39;4008&#39;, &#39;title&#39;: &#39;Historisk krimi&#39;, &#39;position&#39;: 7, &#39;slugname&#39;: &#39;historisk-krimi-2&#39;}]} . I am interested in the slugnames of the main categories. These are the names I need to use in order to get books from each category. . slugs = [] for bookcat in bookcats: slugs.append(bookcat[&#39;slugname&#39;]) . slugs . [&#39;krimi-1&#39;, &#39;spaending-1&#39;, &#39;sande-historier-1&#39;, &#39;borneboger-1&#39;, &#39;feelgood-and-romance-1&#39;, &#39;biografier-and-reportage-1&#39;, &#39;skonlitteratur-1&#39;, &#39;skraek-1&#39;, &#39;klassikere-and-poesi-1&#39;, &#39;fantasy-and-science-fiction-1&#39;, &#39;personlig-udvikling-1&#39;, &#39;fakta-1&#39;, &#39;samfund-and-politik-1&#39;, &#39;serier-and-humor-1&#39;, &#39;livsstil-and-hobby-1&#39;, &#39;teenager-and-young-adult-1&#39;, &#39;letlaest-1&#39;, &#39;books-in-english&#39;] . With this list of categories, it is time to query each one and get books. For that I will use my get_books function. I will try to get 1,000 books from each category. . books_list = [] for slug in tqdm(slugs): try: books = get_books(slug, 1000) df = pd.DataFrame(books) df[&#39;category&#39;] = slug books_list.append(df) except Exception as err: print(f&#39;An error occurred with {slug}: {err}&#39;) . 100%|██████████| 18/18 [01:01&lt;00:00, 3.42s/it] . This gives me a list of pandas dataframes for each column that I will concatenate into one big data frame. . booksdf = pd.concat(books_list) booksdf.head(5) . id title type imageurl weburl descriptionbrief relatedbookid pubdate authors allowedinlibrary ... titleslug narratorslug inCompletedList relatedInCompletedList bookstatus relatedbookstatus numberinseries seriesslug series category . 0 10370882 | Alt det som ingen ser | 2 | https://www.nextory.se/coverimg/130/10370882_2... | https://www.nextory.dk/bog/alt-det-som-ingen-s... | Danske Rasmus vågner i en seng i Moskva sammen... | 10374229 | 2019-06-06 00:00:00 +0200 | [Jan Have Eriksen] | 1 | ... | alt-det-som-ingen-ser-10370882 | paul-becker-400331 | 0 | 0 | ACTIVE | ACTIVE | NaN | NaN | NaN | krimi-1 | . 1 10471409 | Bag lukkede døre | 2 | https://www.nextory.se/coverimg/130/10471409_2... | https://www.nextory.dk/bog/bag-lukkede-døre-10... | #1 PÅ SUNDAY TIMES-BESTSELLERLISTE. Kender du ... | 10471545 | 2017-01-31 00:00:00 +0100 | [B.A. Paris] | 1 | ... | bag-lukkede-døre-10471409 | bolette-schrøder-400250 | 0 | 0 | ACTIVE | ACTIVE | 1.0 | mørke-hemmeligheder-20092 | Mørke hemmeligheder | krimi-1 | . 2 10502808 | Hvor flodkrebsene synger | 2 | https://www.nextory.se/coverimg/130/10502808_2... | https://www.nextory.dk/bog/hvor-flodkrebsene-s... | Kya Clark er den vilde pige, ”marskpigen”. Hun... | 10502806 | 2019-09-27 00:00:00 +0200 | [Delia Owens] | 1 | ... | hvor-flodkrebsene-synger-10502808 | sara-ullner-415618 | 0 | 0 | ACTIVE | ACTIVE | NaN | NaN | NaN | krimi-1 | . 3 10528019 | Offer 2117 | 2 | https://www.nextory.se/coverimg/130/10528019_2... | https://www.nextory.dk/bog/offer-2117-10528019 | Offer 2117 er ottende bind i Jussi Adler-Olsen... | 10664393 | 2019-06-14 00:00:00 +0200 | [Jussi Adler-Olsen] | 1 | ... | offer-2117-10528019 | githa-lehrmann-400308 | 0 | 0 | ACTIVE | ACTIVE | 8.0 | afdeling-q-18468 | Afdeling Q | krimi-1 | . 4 10471424 | Sammenbruddet | 2 | https://www.nextory.se/coverimg/130/10471424_2... | https://www.nextory.dk/bog/sammenbruddet-10471424 | Cass har haft det svært siden den aften, hun s... | 10471502 | 2018-08-24 00:00:00 +0200 | [B.A. Paris] | 1 | ... | sammenbruddet-10471424 | bolette-schrøder-400250 | 0 | 0 | ACTIVE | ACTIVE | 2.0 | mørke-hemmeligheder-20092 | Mørke hemmeligheder | krimi-1 | . 5 rows × 29 columns . Find the best books . First I want to reemove duplicates in my data. A book can belong to multiple categories, but I only want one category per book. To do this I look at each isbn number at pick the category where that book has the highest position. If there are ties after that I just pick the first one. Then I drop the rank and position columns. . idx = booksdf.groupby([&#39;isbn&#39;])[&#39;position&#39;].transform(max) == booksdf[&#39;position&#39;] booksunique = booksdf[idx] booksunique = booksunique.groupby(&#39;isbn&#39;).first() booksunique = booksunique.drop([&#39;rank&#39;, &#39;position&#39;], axis=1) booksunique.head(5) . id title type imageurl weburl descriptionbrief relatedbookid pubdate authors allowedinlibrary ... titleslug narratorslug inCompletedList relatedInCompletedList bookstatus relatedbookstatus numberinseries seriesslug series category . isbn . 9780007227761 10051841 | The Hobbit | 2 | https://www.nextory.se/coverimg/130/10051841_2... | https://www.nextory.dk/bog/the-hobbit-10051841 | Bilbo Baggins is a hobbit who enjoys a comfort... | | 2005-10-17 00:00:00 +0200 | [J. R. R. Tolkien] | 1 | ... | the-hobbit-10051841 | martin-shaw-86014 | 0 | 0 | ACTIVE | NaN | NaN | NaN | NaN | books-in-english | . 9780007228386 10051842 | The Fellowship of the Ring (The Lord of the Ri... | 2 | https://www.nextory.se/coverimg/130/10051842_2... | https://www.nextory.dk/bog/the-fellowship-of-t... | Continuing the story begun in The Hobbit, this... | | 2005-10-17 00:00:00 +0200 | [J. R. R. Tolkien] | 1 | ... | the-fellowship-of-the-ring-the-lord-of-the-rin... | rob-inglis-85977 | 0 | 0 | ACTIVE | NaN | 1.0 | the-lord-of-the-rings-2356 | The Lord of the Rings | books-in-english | . 9780007228393 10051850 | The Two Towers (The Lord of the Rings, Book 2) | 2 | https://www.nextory.se/coverimg/130/10051850_2... | https://www.nextory.dk/bog/the-two-towers-the-... | Building on the story begun in The Hobbit and ... | | 2005-10-17 00:00:00 +0200 | [J. R. R. Tolkien] | 1 | ... | the-two-towers-the-lord-of-the-rings-book-2--1... | rob-inglis-85977 | 0 | 0 | ACTIVE | NaN | 2.0 | the-lord-of-the-rings-2356 | The Lord of the Rings | books-in-english | . 9780007237494 10052538 | A Dance With Dragons (A Song of Ice and Fire, ... | 2 | https://www.nextory.se/coverimg/130/10052538_8... | https://www.nextory.dk/bog/a-dance-with-dragon... | HBO’s hit series A GAME OF THRONES is based on... | | 2011-07-12 00:00:00 +0200 | [George R.R. Martin] | 1 | ... | a-dance-with-dragons-a-song-of-ice-and-fire-bo... | roy-dotrice-95418 | 0 | 0 | ACTIVE | NaN | 5.0 | a-song-of-ice-and-fire-2394 | A Song of Ice and Fire | books-in-english | . 9780007237500 10052539 | A Game of Thrones (A Song of Ice and Fire, Boo... | 2 | https://www.nextory.se/coverimg/130/10052539_7... | https://www.nextory.dk/bog/a-game-of-thrones-a... | HBO’s hit series A GAME OF THRONES is based on... | | 2011-07-12 00:00:00 +0200 | [George R.R. Martin] | 1 | ... | a-game-of-thrones-a-song-of-ice-and-fire-book-... | roy-dotrice-95418 | 0 | 0 | ACTIVE | NaN | 1.0 | a-song-of-ice-and-fire-2394 | A Song of Ice and Fire | books-in-english | . 5 rows × 26 columns . Now I want to find the best books. I am using my own logic here and you are welcome to do it in another way - but please share that with me so I can learn. . First I am removing all books without a review. Then I am calculating a log value of number of ratings in order to give good books with fewer rating a better chance. . Then I multiply the average rating with the log of number of ratings to produce a score. The logic is that product of the two is the best indicator of a good book - ie one that consistently generates possitive reviews. . Finally I sort them according to their new score. . booksunique = booksunique[booksunique[&#39;avgrate&#39;] != 0] booksunique = booksunique[booksunique[&#39;numberofrates&#39;] != 0] booksunique[&#39;log_of_numberofrates&#39;] = booksunique[&#39;numberofrates&#39;].apply(np.log) booksunique[&#39;score&#39;] = booksunique[&#39;avgrate&#39;] * booksunique[&#39;log_of_numberofrates&#39;] booksunique = booksunique.sort_values(by = [&#39;score&#39;], ascending = False) . Finally I will clean up the data frame a bit. I will limit the amount of columns and clean the category name. And then finally only pick the top ten in each category. . goodbooks = booksunique[[&#39;title&#39;, &#39;category&#39;, &#39;avgrate&#39;, &#39;numberofrates&#39;, &#39;score&#39;]] goodbooks[&#39;category&#39;] = goodbooks[&#39;category&#39;].apply(clean_cat) # get dataframe sorted by score in each category goodbooks = goodbooks.groupby([&quot;category&quot;]).apply(lambda x: x.sort_values([&quot;score&quot;], ascending = False)).reset_index(drop=True) # select top N rows within each category goodbooks = goodbooks.groupby(&#39;category&#39;).head(10) goodbooks . &lt;ipython-input-29-691811bd2daf&gt;:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy goodbooks[&#39;category&#39;] = goodbooks[&#39;category&#39;].apply(clean_cat) . title category avgrate numberofrates score . 0 Sygeplejersken - En af Danmarkshistoriens mest... | Biografier And Reportage | 4.443798 | 1032 | 30.836646 | . 1 Geggo: Fra vild teenager til businesskvinde og... | Biografier And Reportage | 4.424134 | 837 | 29.773642 | . 2 Rødder: En gangsters udvej. Nedim Yasars historie | Biografier And Reportage | 4.358811 | 471 | 26.827863 | . 3 Kære Zoe Ukhona | Biografier And Reportage | 4.695817 | 263 | 26.165818 | . 4 Nicklas Bendtner - Begge sider | Biografier And Reportage | 4.347059 | 340 | 25.338769 | . ... ... | ... | ... | ... | ... | . 5074 Twilight: Tusmørke | Teenager And Young Adult | 4.352941 | 17 | 12.332811 | . 5075 Råddenskab: Ravneringene 2 | Teenager And Young Adult | 4.461538 | 13 | 11.443620 | . 5076 Evnen: Ravneringene 3 | Teenager And Young Adult | 4.727273 | 11 | 11.335505 | . 5077 Døde piger lyver ikke | Teenager And Young Adult | 4.416667 | 12 | 10.975004 | . 5078 Ravnenes hvisken Bog 1 | Teenager And Young Adult | 4.700000 | 10 | 10.822150 | . 180 rows × 5 columns . FIN . And voila. Now you have a list of pretty good audiobooks to pass the time with. Enjoy :) .",
            "url": "https://mikkelkrogsholm.github.io/fastblog/audiobooks/api/python/jupyter/2020/07/27/best_audio_books.html",
            "relUrl": "/audiobooks/api/python/jupyter/2020/07/27/best_audio_books.html",
            "date": " • Jul 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://mikkelkrogsholm.github.io/fastblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Data scientist . I am a full stack senior data scientist at Teradata. I use data to look for creative and new ways to solve our clients problems. There is data all around us and I love it when we show just what kind of insights can be derived from using and combining data in new an innovative ways. . Open source contributor . I have benefited tremendously from open source software and packages that other people have written. That is why I also try to contribute what I can to the open source community. I currently have authored six official packages for R that are on CRAN and my github has many more unofficial packages. . Entrepreneur . There are thousands of great ideas and opportunities in the world. I combine new ideas every day in my work as a data scientist: by combining new technologies, new algorithms and new methods. And some of these combinations I even turn in to new companies. That is why I founded 56north and Turbolex on the side. . Political scientist . Society is fascinating. Especially the political part. Traditionally trained as a political scientist I apply my data science and entrepreneurial skillset to political issues. I collect data and write analysis and I help political candidates during election campaigns. . Public Speaker . I cannot shut up about the stuff I do. So I often speak at meetups and conferences on my ideas and on new ways of doing things. . Festival lover . I love music, concerts and festivals. I attend a lot of them… A lot. I have also combined my love for data with my love for festivals: I head the data science team at Smukfest (Denmarks second largest festival). We spin up servers, docker containers, algorithms and dashboards that hum away while the music is playing providing valuable insights to the festival management. .",
          "url": "https://mikkelkrogsholm.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Contact",
          "content": "56north | Ginnungagap . CVR: 38700766 . Mikkel Freltoft Krogsholm . Amager Boulevard 126 st th DK-2300 København S . +45 3033 3204 mikkel [at] 56n.dk . linkedin.com/in/mikkelkrogsholm .",
          "url": "https://mikkelkrogsholm.github.io/fastblog/contact/",
          "relUrl": "/contact/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mikkelkrogsholm.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}